#!/usr/bin/env python3
"""
æµ‹è¯•æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿçš„åŠŸèƒ½ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾èµ–psutilï¼‰
"""

import asyncio
import time
import logging
from datetime import datetime
from typing import Dict, Any
from dataclasses import dataclass, field
from collections import defaultdict
import threading
import functools

# ç®€åŒ–çš„ç³»ç»ŸæŒ‡æ ‡ç±»ï¼ˆä¸ä¾èµ–psutilï¼‰
@dataclass
class SystemMetrics:
    timestamp: datetime
    cpu_percent: float = 0.0
    memory_percent: float = 0.0
    disk_io_read: int = 0
    disk_io_write: int = 0
    network_io_sent: int = 0
    network_io_recv: int = 0
    connections: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp.isoformat(),
            'cpu_percent': self.cpu_percent,
            'memory_percent': self.memory_percent,
            'disk_io_read': self.disk_io_read,
            'disk_io_write': self.disk_io_write,
            'network_io_sent': self.network_io_sent,
            'network_io_recv': self.network_io_recv,
            'connections': self.connections
        }

# ç®€åŒ–çš„æ€§èƒ½æŒ‡æ ‡ç±»
@dataclass
class PerformanceMetrics:
    execution_times: list = field(default_factory=list)
    retry_counts: Dict[str, int] = field(default_factory=dict)
    error_types: Dict[str, int] = field(default_factory=dict)
    throughput_history: list = field(default_factory=list)
    latency_percentiles: Dict[str, float] = field(default_factory=dict)
    system_metrics: list = field(default_factory=list)
    
    def calculate_percentiles(self) -> Dict[str, float]:
        if not self.execution_times:
            return {}
        
        sorted_times = sorted(self.execution_times)
        n = len(sorted_times)
        
        return {
            'p50': sorted_times[int(n * 0.5)],
            'p90': sorted_times[int(n * 0.9)],
            'p95': sorted_times[int(n * 0.95)],
            'p99': sorted_times[int(n * 0.99)] if n > 1 else sorted_times[0]
        }
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'execution_times': self.execution_times,
            'retry_counts': dict(self.retry_counts),
            'error_types': dict(self.error_types),
            'throughput_history': self.throughput_history,
            'latency_percentiles': self.latency_percentiles,
            'system_metrics': [metric.to_dict() for metric in self.system_metrics]
        }

# ç®€åŒ–çš„ç³»ç»Ÿç›‘æ§å™¨ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
class SystemMonitor:
    def __init__(self, interval: float = 1.0):
        self.interval = interval
        self.running = False
        self.thread = None
        self.metrics_history = []
        self.start_time = None
    
    def start(self):
        if self.running:
            return
        
        self.running = True
        self.start_time = datetime.now()
        self.thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.thread.start()
        print(f"ç³»ç»Ÿç›‘æ§å·²å¯åŠ¨ï¼Œé—´éš”: {self.interval}ç§’")
    
    def stop(self):
        if not self.running:
            return
        
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        print("ç³»ç»Ÿç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        while self.running:
            metrics = self._collect_metrics()
            self.metrics_history.append(metrics)
            time.sleep(self.interval)
    
    def _collect_metrics(self) -> SystemMetrics:
        # æ¨¡æ‹Ÿç³»ç»ŸæŒ‡æ ‡æ•°æ®
        import random
        return SystemMetrics(
            timestamp=datetime.now(),
            cpu_percent=random.uniform(10, 80),
            memory_percent=random.uniform(30, 70),
            disk_io_read=random.randint(1000, 10000),
            disk_io_write=random.randint(500, 5000),
            network_io_sent=random.randint(100, 1000),
            network_io_recv=random.randint(200, 2000),
            connections=random.randint(5, 50)
        )
    
    def get_average_metrics(self) -> SystemMetrics:
        if not self.metrics_history:
            return SystemMetrics(timestamp=datetime.now())
        
        # è®¡ç®—å¹³å‡å€¼
        avg_cpu = sum(m.cpu_percent for m in self.metrics_history) / len(self.metrics_history)
        avg_memory = sum(m.memory_percent for m in self.metrics_history) / len(self.metrics_history)
        total_disk_read = sum(m.disk_io_read for m in self.metrics_history)
        total_disk_write = sum(m.disk_io_write for m in self.metrics_history)
        total_net_sent = sum(m.network_io_sent for m in self.metrics_history)
        total_net_recv = sum(m.network_io_recv for m in self.metrics_history)
        avg_connections = sum(m.connections for m in self.metrics_history) / len(self.metrics_history)
        
        return SystemMetrics(
            timestamp=datetime.now(),
            cpu_percent=avg_cpu,
            memory_percent=avg_memory,
            disk_io_read=total_disk_read,
            disk_io_write=total_disk_write,
            network_io_sent=total_net_sent,
            network_io_recv=total_net_recv,
            connections=avg_connections
        )

# æŒ‡æ ‡æ”¶é›†å™¨
class MetricsCollector:
    def __init__(self):
        self.execution_times = []
        self.retry_counts = defaultdict(int)
        self.error_types = defaultdict(int)
        self.throughput_history = []
        self.workflow_results = []
    
    def record_workflow_result(self, result):
        self.workflow_results.append(result)
        
        if hasattr(result, 'execution_time'):
            self.execution_times.append(result.execution_time)
        
        if hasattr(result, 'retry_count'):
            workflow_id = getattr(result, 'workflow_id', 'unknown')
            self.retry_counts[workflow_id] = result.retry_count
        
        if hasattr(result, 'error_message') and result.error_message:
            self.error_types['æ¨¡æ‹Ÿå¤±è´¥'] += 1
    
    def record_batch_throughput(self, throughput: float):
        self.throughput_history.append(throughput)
    
    def generate_performance_metrics(self, system_metrics: list = None) -> PerformanceMetrics:
        metrics = PerformanceMetrics(
            execution_times=self.execution_times.copy(),
            retry_counts=dict(self.retry_counts),
            error_types=dict(self.error_types),
            throughput_history=self.throughput_history.copy(),
            system_metrics=system_metrics or []
        )
        
        metrics.latency_percentiles = metrics.calculate_percentiles()
        return metrics
    
    def reset(self):
        self.execution_times.clear()
        self.retry_counts.clear()
        self.error_types.clear()
        self.throughput_history.clear()
        self.workflow_results.clear()

# æ—¶é—´è£…é¥°å™¨
def timing_decorator(func):
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            execution_time = time.time() - start_time
            
            # å¦‚æœç»“æœæœ‰execution_timeå±æ€§ï¼Œæ›´æ–°å®ƒ
            if hasattr(result, 'execution_time'):
                result.execution_time = execution_time
            
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            return {
                'error': e,
                'execution_time': execution_time,
                'success': False
            }
    return wrapper

# å·¥ä½œæµç»“æœç±»
@dataclass
class WorkflowResult:
    workflow_id: str
    success: bool
    execution_time: float
    submitted_at: datetime
    completed_at: datetime
    retry_count: int = 0
    error_message: str = None

# æµ‹è¯•é…ç½®ç±»
@dataclass
class TestConfig:
    enable_system_monitoring: bool = True
    monitoring_interval: float = 1.0

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MetricsSystemTester:
    """æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config = TestConfig(
            enable_system_monitoring=True,
            monitoring_interval=0.5  # æ›´é¢‘ç¹çš„ç›‘æ§ç”¨äºæµ‹è¯•
        )
        self.system_monitor = SystemMonitor(interval=self.config.monitoring_interval)
        self.metrics_collector = MetricsCollector()
    
    @timing_decorator
    async def simulate_workflow_execution(self, workflow_id: str, duration: float, should_fail: bool = False) -> WorkflowResult:
        """æ¨¡æ‹Ÿå·¥ä½œæµæ‰§è¡Œ"""
        logger.info(f"å¼€å§‹æ¨¡æ‹Ÿå·¥ä½œæµ {workflow_id}ï¼Œé¢„æœŸè€—æ—¶ {duration}s")
        
        start_time = time.time()
        await asyncio.sleep(duration)  # æ¨¡æ‹Ÿå·¥ä½œæµæ‰§è¡Œæ—¶é—´
        
        if should_fail:
            result = WorkflowResult(
                workflow_id=workflow_id,
                success=False,
                execution_time=duration,
                error_message="æ¨¡æ‹Ÿå¤±è´¥",
                submitted_at=datetime.now(),
                completed_at=datetime.now(),
                retry_count=1
            )
        else:
            result = WorkflowResult(
                workflow_id=workflow_id,
                success=True,
                execution_time=duration,
                submitted_at=datetime.now(),
                completed_at=datetime.now(),
                retry_count=0
            )
        
        return result
    
    async def test_system_monitoring(self):
        """æµ‹è¯•ç³»ç»Ÿç›‘æ§åŠŸèƒ½"""
        logger.info("ğŸ” æµ‹è¯•ç³»ç»Ÿç›‘æ§åŠŸèƒ½...")
        
        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        self.system_monitor.start()
        logger.info("âœ… ç³»ç»Ÿç›‘æ§å·²å¯åŠ¨")
        
        # è¿è¡Œä¸€æ®µæ—¶é—´æ”¶é›†æ•°æ®
        await asyncio.sleep(3)
        
        # åœæ­¢ç›‘æ§å¹¶è·å–å¹³å‡æŒ‡æ ‡
        self.system_monitor.stop()
        avg_metrics = self.system_monitor.get_average_metrics()
        
        if avg_metrics:
            logger.info(f"ğŸ“Š å¹³å‡ç³»ç»ŸæŒ‡æ ‡:")
            logger.info(f"   CPU: {avg_metrics.cpu_percent:.1f}%")
            logger.info(f"   å†…å­˜: {avg_metrics.memory_percent:.1f}%")
            logger.info(f"   è¿æ¥æ•°: {avg_metrics.connections:.0f}")
            logger.info(f"   ç›‘æ§å†å²è®°å½•æ•°: {len(self.system_monitor.metrics_history)}")
        else:
            logger.warning("âŒ æœªèƒ½è·å–ç³»ç»ŸæŒ‡æ ‡")
        
        return avg_metrics is not None
    
    async def test_metrics_collection(self):
        """æµ‹è¯•æŒ‡æ ‡æ”¶é›†åŠŸèƒ½"""
        logger.info("ğŸ“ˆ æµ‹è¯•æŒ‡æ ‡æ”¶é›†åŠŸèƒ½...")
        
        # é‡ç½®æŒ‡æ ‡æ”¶é›†å™¨
        self.metrics_collector.reset()
        
        # æ¨¡æ‹Ÿå¤šä¸ªå·¥ä½œæµæ‰§è¡Œ
        workflows = [
            ("workflow-1", 0.5, False),
            ("workflow-2", 1.0, False),
            ("workflow-3", 0.3, True),   # å¤±è´¥çš„å·¥ä½œæµ
            ("workflow-4", 0.8, False),
            ("workflow-5", 0.2, True),   # å¦ä¸€ä¸ªå¤±è´¥çš„å·¥ä½œæµ
        ]
        
        for workflow_id, duration, should_fail in workflows:
            result = await self.simulate_workflow_execution(workflow_id, duration, should_fail)
            self.metrics_collector.record_workflow_result(result)
        
        # è®°å½•æ‰¹æ¬¡ååé‡
        self.metrics_collector.record_batch_throughput(2.5)  # 5ä¸ªå·¥ä½œæµ / 2ç§’
        
        # ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡
        performance_metrics = self.metrics_collector.generate_performance_metrics()
        
        # éªŒè¯æŒ‡æ ‡
        logger.info(f"ğŸ“Š æ”¶é›†çš„æ€§èƒ½æŒ‡æ ‡:")
        logger.info(f"   æ‰§è¡Œæ—¶é—´è®°å½•æ•°: {len(performance_metrics.execution_times)}")
        logger.info(f"   é‡è¯•æ¬¡æ•°è®°å½•æ•°: {len(performance_metrics.retry_counts)}")
        logger.info(f"   é”™è¯¯ç±»å‹: {dict(performance_metrics.error_types)}")
        logger.info(f"   ååé‡å†å²: {performance_metrics.throughput_history}")
        
        if performance_metrics.latency_percentiles:
            logger.info(f"   å»¶è¿Ÿç™¾åˆ†ä½æ•°:")
            for percentile, value in performance_metrics.latency_percentiles.items():
                logger.info(f"     {percentile}: {value:.3f}s")
        
        # éªŒè¯æ•°æ®æ­£ç¡®æ€§
        expected_workflows = len(workflows)
        actual_workflows = len(performance_metrics.execution_times)
        
        if actual_workflows == expected_workflows:
            logger.info("âœ… æŒ‡æ ‡æ”¶é›†æ•°é‡æ­£ç¡®")
        else:
            logger.error(f"âŒ æŒ‡æ ‡æ”¶é›†æ•°é‡é”™è¯¯: æœŸæœ› {expected_workflows}, å®é™… {actual_workflows}")
            return False
        
        # éªŒè¯é”™è¯¯ç»Ÿè®¡
        failed_count = sum(1 for _, _, should_fail in workflows if should_fail)
        recorded_errors = sum(performance_metrics.error_types.values())
        
        if recorded_errors == failed_count:
            logger.info("âœ… é”™è¯¯ç»Ÿè®¡æ­£ç¡®")
        else:
            logger.error(f"âŒ é”™è¯¯ç»Ÿè®¡é”™è¯¯: æœŸæœ› {failed_count}, å®é™… {recorded_errors}")
            return False
        
        return True
    
    async def test_timing_decorator(self):
        """æµ‹è¯•æ—¶é—´è£…é¥°å™¨åŠŸèƒ½"""
        logger.info("â±ï¸ æµ‹è¯•æ—¶é—´è£…é¥°å™¨åŠŸèƒ½...")
        
        @timing_decorator
        async def test_function(duration: float):
            await asyncio.sleep(duration)
            return f"å®Œæˆï¼Œè€—æ—¶ {duration}s"
        
        # æµ‹è¯•æ­£å¸¸æ‰§è¡Œ
        start_time = time.time()
        result = await test_function(0.5)
        actual_duration = time.time() - start_time
        
        logger.info(f"å‡½æ•°è¿”å›: {result}")
        logger.info(f"å®é™…æ‰§è¡Œæ—¶é—´: {actual_duration:.3f}s")
        
        # éªŒè¯æ—¶é—´æµ‹é‡çš„å‡†ç¡®æ€§ï¼ˆå…è®¸ä¸€å®šè¯¯å·®ï¼‰
        if 0.4 <= actual_duration <= 0.6:
            logger.info("âœ… æ—¶é—´è£…é¥°å™¨å·¥ä½œæ­£å¸¸")
            return True
        else:
            logger.error(f"âŒ æ—¶é—´è£…é¥°å™¨æµ‹é‡ä¸å‡†ç¡®")
            return False
    
    async def test_json_serialization(self):
        """æµ‹è¯•JSONåºåˆ—åŒ–åŠŸèƒ½"""
        logger.info("ğŸ”„ æµ‹è¯•JSONåºåˆ—åŒ–åŠŸèƒ½...")
        
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            result = WorkflowResult(
                workflow_id="test-workflow",
                success=True,
                execution_time=1.5,
                submitted_at=datetime.now(),
                completed_at=datetime.now()
            )
            
            self.metrics_collector.record_workflow_result(result)
            performance_metrics = self.metrics_collector.generate_performance_metrics()
            
            # æµ‹è¯•åºåˆ—åŒ–
            metrics_dict = performance_metrics.to_dict()
            logger.info(f"âœ… PerformanceMetrics åºåˆ—åŒ–æˆåŠŸ")
            logger.info(f"   åºåˆ—åŒ–å­—æ®µæ•°: {len(metrics_dict)}")
            
            # å¯åŠ¨ç³»ç»Ÿç›‘æ§è·å–ç³»ç»ŸæŒ‡æ ‡
            self.system_monitor.start()
            await asyncio.sleep(1)
            self.system_monitor.stop()
            
            avg_metrics = self.system_monitor.get_average_metrics()
            if avg_metrics:
                system_dict = avg_metrics.to_dict()
                logger.info(f"âœ… SystemMetrics åºåˆ—åŒ–æˆåŠŸ")
                logger.info(f"   åºåˆ—åŒ–å­—æ®µæ•°: {len(system_dict)}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ JSONåºåˆ—åŒ–å¤±è´¥: {e}")
            return False
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿç»¼åˆæµ‹è¯•")
        
        tests = [
            ("ç³»ç»Ÿç›‘æ§", self.test_system_monitoring),
            ("æŒ‡æ ‡æ”¶é›†", self.test_metrics_collection),
            ("æ—¶é—´è£…é¥°å™¨", self.test_timing_decorator),
            ("JSONåºåˆ—åŒ–", self.test_json_serialization),
        ]
        
        results = []
        for test_name, test_func in tests:
            logger.info(f"\n--- æµ‹è¯•: {test_name} ---")
            try:
                result = await test_func()
                results.append((test_name, result))
                if result:
                    logger.info(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
                else:
                    logger.error(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
            except Exception as e:
                logger.error(f"ğŸ’¥ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
                results.append((test_name, False))
        
        # æ€»ç»“æµ‹è¯•ç»“æœ
        logger.info("\nğŸ¯ æµ‹è¯•ç»“æœæ€»ç»“:")
        passed = sum(1 for _, result in results if result)
        total = len(results)
        
        for test_name, result in results:
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            logger.info(f"   {test_name}: {status}")
        
        logger.info(f"\næ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡ ({passed/total*100:.1f}%)")
        
        if passed == total:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿå·¥ä½œæ­£å¸¸ã€‚")
        else:
            logger.warning(f"âš ï¸ æœ‰ {total-passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
        
        return passed == total


async def main():
    """ä¸»å‡½æ•°"""
    tester = MetricsSystemTester()
    success = await tester.run_all_tests()
    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)